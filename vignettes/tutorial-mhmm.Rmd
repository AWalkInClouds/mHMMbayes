---
title: "Multilevel HMM tutorial"
author: 
  name: "Emmeke Aarts"
  affiliation: "Department of Methodology and Statistics, Utrecht University, Utrecht, the Netherlands <br>"
abstract: >
  Document summary n Markov models [HMMs; @Rabiner1989] are a machine learning method that have been used in many different scientific fields for several decades. They are used to analyze a long sequence of data, such as a string of DNA REFS or a sequence of speech REFS. [INCLUDE MORE REFS] Because 
output: rmarkdown::html_vignette
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Multilevel HMM tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<br> <br>

## Introduction
Hidden Markov models [HMMs; @Rabiner1989] are a machine learning method that have been used in many different scientific fields for several decades. They are used to analyze a long sequence of data, such as a string of DNA REFS or a sequence of speech REFS. [INCLUDE MORE REFS] Because of technological advancements, it becomes increasingly easy to also collect such long sequences of data on behavior as it unfolds in real time. Examples from the social sciences are the interactions between a toddler and a child doing a clean-up task, where emotion and behavior of both the toddler and parent is registered every second for a period of several minutes, or the interaction between a therapist and a patient, where different types of nonverbal communication are registered every second for a period of 15 minutes. When applying these models to real time behavioral data, they can be used to extract latent behavioral states over time, and model the dynamics of behavior over time (that is, the dynamics between these latent states). \
A quite recent development herein, is the extension to multilevel HMMs [REFS]. Using the multilevel framework, we can model several sequences (e.g., sequence of different persons) simultaneously, while accomodating the heterogeneity between persons. Using the multilevel framwerok, we can quantify the amount of heterogeniety between persons in their dynimics of behavior, easily perform group comparisons on the model parameters, or investigate how model parameters change as a result of a covariate. For example, are the dynamics between a patient and a therapist different for patients with a good therapeutic outcome and patients with a less favorable therapeutic outcome? \
This package Xx. For a gentle introduction to HMMs, we refer to xx. For multilevel xx. Information on the used estimation methods is given in the [estimation vignette](#estimation-mhmm.html). 

## Hidden Markov models
The Hidden Markov Model is used 1) to infer latent or hidden states, which are defined by the probability to observe (categorical) outcomes, and 2) to account for the dynamics of the observed outcomes in term of the dynamics of the hidden states [see e.g., @Rabiner1989, @ephraim2002, @cappe2005, @zucchini2016]. The former is based on the assumption that a given observed categorical outcome $O_t$ in the sequence $\{O_t: t = 1, 2, \ldots, T\}$ is generated by an underlying, latent state $S_t$. The latter is based on the assumption that the sequence of hidden states $\{S_t: t = 1, 2, \ldots, T\}$ forms a  Markov chain. The HMM is a discrete time model: for each point in time $t$, we have one hidden state that generates one observed event for that time point $t$. \
The probability of observing the current event $O_t$ is exclusively determined by the current latent state $S_t$:
\begin{equation}
Pr(O_{t} \mid \ O_{t-1}, O_{t-2}, \ldots, O_{1}, \ S_{t}, S_{t-1}, \ldots, S_{1}) = Pr(O_{t} \mid S_{t}).
\end{equation}
The probability of observing $O_t$ given $S_t$ can have any distribution, e.g., discrete or continuous. In the current version of the package `mHMMbayes`, only the categorical emission distribution is implemented. \

The hidden states in the sequence take values from a countable finite set of states $S_t = i, i \in \{1, 2, \ldots, m\}$, where $m$ denotes the number of distinct states, that form the Markov chain, with the Markov property:  
\begin{equation}
Pr(S_{t+1} \mid \ S_{t}, S_{t-1}, \ldots, S_{1}) = Pr(S_{t+1} \mid S_{t}).
\end{equation}
That is, the probability of switching to the next state $S_{t+1}$ depends only on the current state $S_t$. As the HMM is a discrete time model, the duration of a state is represented by the self-transition probabilities $\gamma_{ii}$, where the probability of a certain time t spent in state $S$ is given by the geometric distribution: $\gamma_{ii}^{t-1}(1-\gamma_{ii})$. \
The discrete time HMM includes three sets of parameters: the initial probabilities of the states $\pi_i$, the matrix $\mathbf{\Gamma}$ including the transition probabilities $\gamma_{ij}$ between the states, and the state-dependent probability distribution of observing $O_t$ given $S_t$ with parameter set $\boldsymbol{\theta}_i$.  The initial probabilities $\pi_i$ denote the probability that the first state in the hidden state sequence, $S_1$, is $i$: 
\begin{equation}
\pi_i = Pr(S_1 = i) \quad \text{with} \sum_i \pi_i = 1. 
\end{equation}
Often, the initial probabilities of the states $\pi_i$ are assumed to be the stationary distribution implied by the transition probability matrix $\mathbf{\Gamma}$, that is, the long term steady-state probabilities obtained by $\lim_{T \rightarrow \infty} \mathbf{\Gamma}^T$. The transition probability matrix $\mathbf{\Gamma}$ with transition probabilities $\gamma_{ij}$ denote the probability of switching from state $i$ at time $t$ to state $j$ at time $t+1$:
\begin{equation}
\gamma_{ij} = Pr(S_{t+1} = j \mid S_{t} = i) \quad \text{with} \sum_j \gamma_{ij} = 1.
\end{equation}
That is, the transition probabilities $\gamma_{ij}$ in the HMM represent the probability to switch between hidden states rather than between observed acts, as in the MC and CTMC model. The state-dependent probability distribution denotes the probability of observing $O_t$ given $S_t$ with parameter set $\boldsymbol{\theta}_i$. In case of our mouse data, the state-dependent probability distribution is given by the categorical distribution, and the parameter set $\boldsymbol{\theta}_i$ is the set of state-dependent probabilities of observing acts. That is, 
\begin{equation}
Pr(O_t = o \mid S_t = i) \sim \text{Cat} (\boldsymbol{\theta}_i),
\end{equation}
for the observed outcomes $o = 1, 2, \ldots, q$ and where $\boldsymbol{\theta}_i = (\theta_{i1}, \theta_{i2}, \ldots, \theta_{iq})$ is a vector of probabilities for each state $S = i, \ldots, m$ with $\sum \theta_i = 1$, i.e., within each state, the probabilities of all possible outcomes sum up to 1. \
We assume that all parameters in the HMM are independent of $t$, i.e., we assume a time-homogeneous model. In the [estimation vignette](#estimation-mhmm.html) we discuss three methods (i.e., Maximum likelihood, Expectation Maximization or Baum-Welch algorithm, and Bayesian estimation) to estimate the parameters of an HMM. In the package `mHMMbayes`, we chose to use Bayesian estimation because of its flexibility, which we will require in the multilevel framework of the model. 

###Determining the number of hidden states
The first step in developing a HMM is to determine the number of states $m$ that best describes the observed data, and is a model selection problem. In case of the mouse data, the task is to define the states by clusters of observed behavioral acts that provide a reasonable, biologically interpretable, description of the data. As such, we used a combination of unadjusted and the Bayesian Information Criterion (BIC) adjusted log- likelihoods and the biological interpretability of the estimated states to choose between models. We used both the unadjusted and BIC adjusted log-likelihoods, because the likelihood ratio test, commonly used to compare nested models, cannot be used in case of the HMM (i.e., the difference in the log-likelihoods between models is not $\chi^2$ distributed \cite{ryden2008}). The BIC adjusted log-likelihood provides an alternative to compare the log-likelihoods of various models adjusted for the number of estimated parameters in the model. The BIC is defined as BIC $= -2 \text{log} L + n_p \ \text{log}(T)$, where $L$ is the likelihood of the model, $n_p$ is the number of (freely) estimated parameters, and $T$ is the length of the observed event sequence (see the vignette `Estimation of the multilevel hidden Markov model` for the likelihood function).^[We note, however, that the BIC approximates the posterior distribution of the parameters by a Gaussian distribution, which might not be appropriate for models including parameters on the boundary of the parameter space (e.g., close to 0 or 1 in case of probability estimates), or for small data sets, as exemplified by @scott2002. Model selection is therefore not a straightforward procedure in the context of HMM, and the choices remain subjective.]

###Determining the most likely state sequence 
Given a well-fitting HMM, it may be of interest to determine the actual *sequence*, or order of succession, of hidden states that has most likely given rise to the sequence of outcomes as observed in a subject. One can either use local decoding, in which the probabilities of the hidden state sequence are obtained simultaneously with the model parameters estimates, or the well-known Viterbi algorithm  [@viterbi1967; @forney1973]. In local decoding, the most likely state is determined separately at each time point $t$, in contrast to the Viterbi algorithm in which one determines the joint probability of the complete sequence of observations $O_{1:T}$ and the complete sequence of hidden states $S_{1:T}$. 

## Multilevel hidden Markov models
Given data of multiple subjects, one may fit the HMM to the data of each subject separately, or fit one and the same HMM model to the data of all subject, under the strong (generally untenable) assumption that the subjects do not differ with respect to the parameters of the HMM. Fitting a different model to each behavioral sequence is unparsimonous, computationally intensive, and results in a large number of parameters estimates. Neither approach lends itself well for a formal comparison (e.g., comparing the parameters over experimental conditions). To facilitate the analysis of multiple subjects, the HMM is extended by putting it in a multilevel framework. In multilevel models, model parameters are specified that pertain to different levels in the data. For example, subject-specific model parameters describe the data collected within each subject, and group level parameters describe what is typically observed within the group of subjects and the variation observed between subjects within a group. In our multilevel HMM, we allow each subject to have its own unique parameter values within the same HMM model (i.e., identical number and composition of the hidden states). Rather than estimating these subject-specific parameters individually, we assume that the parameters of the HMM are random, i.e., follow a given group level distribution. Within this multilevel structure, the mean and the variance of the group level distribution of a given parameter thus expresses the overall mean parameter value in a group of subjects and the parameter variability between the subjects in the group. We refer to parameter variability as randomness or heterogeneity. \
Multilevel HMMs have received some attention in the literature. In a frequentist context, @altman2007 presented a general framework for HMMs for multiple processes by defining a class of Mixed Hidden Markov Models (MHMMs). These models are however, computationally intensive and due to slow convergence only suited for modeling a limited number of random effects. The approach of Altman has been translated to the Bayesian framework [see e.g., @rueda2013; @zhang2014; @shirley2010; @deHaan2017], which proved much faster as the time to reach convergence is decreased @Zhang2014. In addition, the HMM in a Bayesian context is easier to adapt to a hierarchical model, as the need for numerical integration is eliminated. The Bayesian mixed HMM has been applied to the analysis of for example DNA copy number data @rueda2013, risk factors for asthma @zhang2014, and a clinical trial of a treatment for alcoholism @shirley2010. \
In the vignette `Estimation of the multilevel hidden Markov model`, we present in detail the multilevel HMM, allowing for heterogeneity in the state transition probability matrix, the state-dependent probabilities, and the state durations. In the multilevel model, we have a multi-layered structure in the parameters: individual level parameters at the first level pertaining to the observations within a subject, and group level parameters at the second level that describe the variation in the subject level parameters in the population as inferred from the group of subjects (see vignette [estimation vignette](#estimation-mhmm.html) for a more detailed discussion on and theoretical example of the multielvel model using Bayesian estimation). This model allows us to incorporate the heterogeneity in the sequences of subjects while including shared information on model parameters, and retaining a reasonable number of model parameters. The subject specific parameters are supplemented with the prefix $k$, denoting subject $k \in \{1,2,\ldots,K\}$. Hence, in the multilevel (bayesian) HMM, the subject specific parameters are: the subject-specific transition probability matrix $\boldsymbol{\Gamma}_k$ with transition probabilities $\gamma_{k,ij}$, and the subject-specific emission distributions denoting subject-specific probabilities $\boldsymbol{\theta}_{k,i}$ of categorical outcomes within hidden state $i$.  The initial probabilities of the states $\pi_{k,j}$ are not estimated as $\pi_{k}$ is assumed to be the stationary distribution of $\boldsymbol{\Gamma}_k$. The group level parameters are: the group level state transition probability matrix  $\boldsymbol{\Gamma}$ with transition probabilities $\gamma_{ij}$, and the group level state-dependent probabilities $\boldsymbol{\theta}_{i}$. \
We fit the model using Bayesian estimation (i.e., a hybrid Metropolis Gibbs sampler that utilizes the forward-backward recursion to sample the hidden state sequence of each subject, see the [estimation vignette](#estimation-mhmm.html)). 

## Using the package mHMMbayes

* example data plus visualization
* running the analysis plus start values, prior
* outcomes of the anlaysis: emission distribution and transition matrix, group means
* individual level parameters, plotting
* number of states
* convergence, using different starting values
* simulating data








Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))

## References

