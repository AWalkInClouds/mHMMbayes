---
title: "Multilevel HMM tutorial"
author: 
  name: "Emmeke Aarts"
  affiliation: "Department of Methodology and Statistics, Utrecht University, Utrecht, the Netherlands <br>"
abstract: >
    With the \code{R} package mHMMbayes you can fit multilevel hidden Markov models. The multilevel hidden Markov model (HMM) is a generalization of the well-known hidden Markov model, tailored to accomodate (intense) longitudinal data of multiple individuals simultaneously. Using a multilevel framework, we allow for heterogeneity in the model parameters (transition probability matrix and conditional distribution), while estimating one overall HMM. The model has a great potential of application in  many fields, such as the social sciences and medicine. The model can be fitted on multivariate data with a catagorical  distribution, and include individual level covariates (allowing for e.g., group comparisons on model parameters). Parameters are estimated using Bayesian estimation utilizing the forward-backward recursion within a hybrid Metropolis within Gibbs sampler. The package also includes a function to simulate data and a function to obtain the most likely hidden state sequence for each individual using the Viterbi algorithm. 
output: 
  rmarkdown::html_vignette:
    mathjax: default
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Multilevel HMM tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
<br> <br>

Note: Equations in the html vignette might not render properly in Rstudio browser. The equations do render correctly when the vignette is opened in a webbrowser window.

## Introduction
Hidden Markov models [HMMs; @Rabiner1989] are a machine learning method that have been used in many different scientific fields for several decades. They are used to analyze a long sequence of data, such as a string of DNA REFS or a sequence of speech REFS. [INCLUDE MORE REFS] The development of this package is, however, motivated from the area of the social sciences. Because of technological advancements, it becomes increasingly easy to also collect such long sequences of data on behavior. That is, we can monitor behavior as it unfolds in real time. An example from the social sciences is the interaction between a therapist and a patient, where different types of nonverbal communication are registered every second for a period of 15 minutes. When applying HMMs to such behavioral data, they can be used to extract latent behavioral states over time, and model the dynamics of behavior over time. \
A quite recent development in HMMs, is the extension to multilevel HMMs [REFS]. Using the multilevel framework, we can model several sequences (e.g., sequence of different persons) simultaneously, while accomodating the heterogeneity between persons. As a result, we can quantify the amount of heterogeniety between persons in their dynimics of behavior, easily perform group comparisons on the model parameters, and investigate how model parameters change as a result of a covariate. For example, are the dynamics between a patient and a therapist different for patients with a good therapeutic outcome and patients with a less favorable therapeutic outcome? \
With the package `mHMMbayes`, one can estimate these multilevel hidden Markov models. This tutorial starts out with a brief description of the HMM and the multilevel HMM. For a more elaborate and gentle introduction to HMMs, we refer to @zucchini2016. Next, we show how to use the package `mHMMbayes` through an extensive example, also touching on the issus of determining the number of hidden states and checking model convergnece. Information on the used estimation methods and algorighms in the package is given in the vignette [Estimation of the multilevel hidden Markov model](estimation-mhmm.pdf). 

## Hidden Markov models
Hidden Markov Models are used for data for which 1) we believe that the distribuiton generating the observation depends on the state of an underlying, hidden state, and 2) the hidden states follow a Markov process, i.e., the states over time are not independent of oneanother, but the current state  depends on the previous state only (and not on earlier states) [see e.g., @Rabiner1989, @ephraim2002, @cappe2005, @zucchini2016].  The HMM is a discrete time model: for each point in time $t$, we have one hidden state that generates one observed event for that time point $t$. \
Hence, the probability of observing the current outcome $O_t$ is exclusively determined by the current latent state $S_t$:
\begin{equation}
Pr(O_{t} \mid \ O_{t-1}, O_{t-2}, \ldots, O_{1}, \ S_{t}, S_{t-1}, \ldots, S_{1}) = Pr(O_{t} \mid S_{t}).
\end{equation}
The probability of observing $O_t$ given $S_t$ can have any distribution, e.g., discrete or continuous. In the current version of the package `mHMMbayes`, only the categorical emission distribution is implemented. \
The hidden states in the sequence take values from a countable finite set of states $S_t = i, i \in \{1, 2, \ldots, m\}$, where $m$ denotes the number of distinct states, that form the Markov chain, with the Markov property:  
\begin{equation}
Pr(S_{t+1} \mid \ S_{t}, S_{t-1}, \ldots, S_{1}) = Pr(S_{t+1} \mid S_{t}).
\end{equation}
That is, the probability of switching to the next state $S_{t+1}$ depends only on the current state $S_t$. As the HMM is a discrete time model, the duration of a state is represented by the self-transition probabilities $\gamma_{ii}$, where the probability of a certain time t spent in state $S$ is given by the geometric distribution: $\gamma_{ii}^{t-1}(1-\gamma_{ii})$. \
The HMM includes three sets of parameters: the initial probabilities of the states $\pi_i$, the matrix $\mathbf{\Gamma}$ including the transition probabilities $\gamma_{ij}$ between the states, and the state-dependent probability distribution of observing $O_t$ given $S_t$ with parameter set $\boldsymbol{\theta}_i$.  The initial probabilities $\pi_i$ denote the probability that the first state in the hidden state sequence, $S_1$, is $i$: 
\begin{equation}
\pi_i = Pr(S_1 = i) \quad \text{with} \sum_i \pi_i = 1. 
\end{equation}
Often, the initial probabilities of the states $\pi_i$ are assumed to be the stationary distribution implied by the transition probability matrix $\mathbf{\Gamma}$, that is, the long term steady-state probabilities obtained by $\lim_{T \rightarrow \infty} \mathbf{\Gamma}^T$. The transition probability matrix $\mathbf{\Gamma}$ with transition probabilities $\gamma_{ij}$ denote the probability of switching from state $i$ at time $t$ to state $j$ at time $t+1$:
\begin{equation}
\gamma_{ij} = Pr(S_{t+1} = j \mid S_{t} = i) \quad \text{with} \sum_j \gamma_{ij} = 1.
\end{equation}
That is, the transition probabilities $\gamma_{ij}$ in the HMM represent the probability to switch between hidden states rather than between observed acts, as in the MC and CTMC model. The state-dependent probability distribution denotes the probability of observing $O_t$ given $S_t$ with parameter set $\boldsymbol{\theta}_i$. In case of the package, the state-dependent probability distribution is given by the categorical distribution, and the parameter set $\boldsymbol{\theta}_i$ is the set of state-dependent probabilities of observing categorical outcomes. That is, 
\begin{equation}
Pr(O_t = o \mid S_t = i) \sim \text{Cat} (\boldsymbol{\theta}_i),
\end{equation}
for the observed outcomes $o = 1, 2, \ldots, q$ and where $\boldsymbol{\theta}_i = (\theta_{i1}, \theta_{i2}, \ldots, \theta_{iq})$ is a vector of probabilities for each state $S = i, \ldots, m$ with $\sum \theta_i = 1$, i.e., within each state, the probabilities of all possible outcomes sum up to 1. \
We assume that all parameters in the HMM are independent of $t$, i.e., we assume a time-homogeneous model. In the vignette [Estimation of the multilevel hidden Markov model](estimation-mhmm.pdf) we discuss three methods (i.e., Maximum likelihood, Expectation Maximization or Baum-Welch algorithm, and Bayesian estimation) to estimate the parameters of an HMM. In the package `mHMMbayes`, we chose to use Bayesian estimation because of its flexibility, which we will require in the multilevel framework of the model. 
 
## Multilevel hidden Markov models
Given data of multiple subjects, one may fit the HMM to the data of each subject separately, or fit one and the same HMM model to the data of all subject, under the strong (generally untenable) assumption that the subjects do not differ with respect to the parameters of the HMM. Fitting a different model to each behavioral sequence is unparsimonous, computationally intensive, and results in a large number of parameters estimates. Neither approach lends itself well for a formal comparison (e.g., comparing the parameters over experimental conditions). To facilitate the analysis of multiple subjects, the HMM is extended by putting it in a multilevel framework. \
In multilevel models, model parameters are specified that pertain to different levels in the data. For example, subject-specific model parameters describe the data collected within each subject, and group level parameters describe what is typically observed within the group of subjects, and the variation observed between subjects. In the implemented multilevel HMM, we allow each subject to have its own unique parameter values within the same HMM model (i.e., identical number and similar composition of the hidden states). Rather than estimating these subject-specific parameters individually, we assume that the parameters of the HMM are random, i.e., follow a given group level distribution. Within this multilevel structure, the mean and the variance of the group level distribution of a given parameter thus expresses the overall mean parameter value in a group of subjects and the parameter variability between the subjects in the group.  \
Multilevel HMMs have received some attention in the literature. In a frequentist context, @altman2007 presented a general framework for HMMs for multiple processes by defining a class of Mixed Hidden Markov Models (MHMMs). These models are however, computationally intensive and due to slow convergence only suited for modeling a limited number of random effects. The approach of Altman has been translated to the Bayesian framework, which proved much faster as the time to reach convergence is decreased @zhang2014. In addition, the HMM in a Bayesian context is easier to adapt to a multilevel model, as the need for numerical integration is eliminated. Examples of the application of the multilevel HMM (within a Bayesian framework) are: @rueda2013 applied the model to the analysis of DNA copy number data, @zhang2014 to identify risk factors for asthma, @shirley2010 to clinical trial data of a treatment for alcoholism and @deHaan2017 to longitudinal data sets in psychology. \
In the tutorial, we use the following notation for the parameters in the multilevel HMM. The subject specific parameters are supplemented with the prefix $k$, denoting subject $k \in \{1,2,\ldots,K\}$. Hence, in the multilevel (bayesian) HMM, the subject specific parameters are: the subject-specific transition probability matrix $\boldsymbol{\Gamma}_k$ with transition probabilities $\gamma_{k,ij}$, and the subject-specific emission distributions denoting subject-specific probabilities $\boldsymbol{\theta}_{k,i}$ of categorical outcomes within hidden state $i$.  The initial probabilities of the states $\pi_{k,j}$ are not estimated as $\pi_{k}$ is assumed to be the stationary distribution of $\boldsymbol{\Gamma}_k$. The group level parameters are: the group level state transition probability matrix  $\boldsymbol{\Gamma}$ with transition probabilities $\gamma_{ij}$, and the group level state-dependent probabilities $\boldsymbol{\theta}_{i}$. We fit the model using Bayesian estimation (i.e., a hybrid Metropolis Gibbs sampler that utilizes the forward-backward recursion to sample the hidden state sequence of each subject, see the vignette [Estimation of the multilevel hidden Markov model](estimation-mhmm.pdf)). 

## Using the package mHMMbayes

We illustrate using the package using the embedded example data `nonverbal`. The data contains the nonverbal communication of 10 patient-therapist couples, recorded for 15 minutes at a frequency of 1 observation per second (= 900 observations per couple). The following variables are contained in the dataset:  

* `id`: id variable of patient - therapis couple to distinguish which observation belongs to which couple.
* `p_verbalizing`: verbalizing behavior of the patient, consisiting of 1 = verbalizing, 2 = back chanelling, 3 = not verbalizing.
* `p_looking`: looking behavior of the patient, consisting of 1 = looking at therapist, 2 = not looking at therapist.
* `t_verbalizing`: verbalizing behavior of the therapist, consisiting of 1 = verbalizing, 2 = back chanelling, 3 = not verbalizing.
* `t_looking`: looking behavior of the therapist, consisting of 1 = looking at patient, 2 = not looking at patient.
The top 6 rows of the dataset are provided below. 

```{r}
library(mHMMbayes)
nonverbal <- data.frame(nonverbal)
head(nonverbal)
```

When we plot the data of the first 5 minutes (= the first 300 observations) of the first couple, we get the following: 

```{r, fig.width = 7.2, fig.height = 4, echo = FALSE}
# set labels and colors for the observed behavioral categorical outcomes
Voc_lab <- c("Speaking", "Back channeling", "Not Speaking")
Look_lab <-  c("Looking", "Not looking")
Voc_col <- c("darkslategray3", "darkslategray4", "gray85")
Look_col <- c("goldenrod1", "gray85")
cols = list(Voc_col, Look_col, Voc_col, Look_col)

time_s  <- seq(1,900)
couple1 <- cbind(nonverbal[nonverbal$id == 1,], time_s)

par(mar = c(4.3, 6.6, 2.1, 1.1))
plot(x = 1, xlim = c(0,300), ylim = c(0.5,6), type = "n", las = 1, xlab = "Time in minutes", xaxt = "n", yaxt = "n", ylab = "")
axis(2, at = seq(1,4), tick = FALSE, labels = c("P_vocalizing", "P_Looking", "T_vocalizing", "T_Looking"), las = 1)
axis(1, at = seq(0,300,60), tick = TRUE, las = 1, labels = FALSE)
axis(1, at = seq(0,300,60), tick = FALSE, las = 1, labels = seq(1,6,1))
abline(v = seq(0,300,60), col = "gray85")

for(j in 2:5){
  for(i in 1:max(nonverbal[,j])){
    points(x = couple1$time_s[1:300][couple1[1:300,j] == i], 
           y = rep(j-1, sum(couple1[1:300,j] == i)), 
           pch = "|", col = cols[[j-1]][i])
  }
}

legend("topright", bty = "n", fill = Voc_col, legend = Voc_lab)
legend("topleft", bty = "n", fill = Look_col, legend = Look_lab)

```

We can for example oberve that both the patient and the therapist are mainly looking at eachother. During the first minute, the patient is mainly speaking. During the second minute, the the therapists, after which the patient takes over while the therapist is backchanneling. 

### Running the analysis



```{r}
# specifying general model properties:
m <- 2
n_dep <- 4
q_emiss <- c(3, 2, 3, 2)

# specifying starting values
start.TM <- diag(.8, m)
start.TM[lower.tri(start.TM) | upper.tri(start.TM)] <- .2
start.EM <- list(matrix(c(0.9, 0.05, 0.05, 0.05, 0.05, 0.9), byrow = TRUE,
                         nrow = m, ncol = q_emiss[1]), # vocalizing patient
                  matrix(c(0.9, 0.1, 0.9, 0.1), byrow = TRUE, nrow = m,
                         ncol = q_emiss[2]), # looking patient
                  matrix(c(0.05, 0.05, 0.9, 0.9, 0.05, 0.05), byrow = TRUE,
                         nrow = m, ncol = q_emiss[3]), # vocalizing therapist
                  matrix(c(0.9, 0.1, 0.9, 0.1), byrow = TRUE, nrow = m,
                         ncol = q_emiss[4])) # looking therapist

# Run a model without covariate(s):
out1 <- mHMM_mnl(s_data = nonverbal, gen = list(m = m, n_dep = n_dep,
                 q_emiss = q_emiss), start_val = c(list(start.TM), start.EM),
                 mcmc = list(J = 11, burn_in = 5))

```



* running the analysis plus start values, prior
* outcomes of the anlaysis: emission distribution and transition matrix, group means
* individual level parameters, plotting
* number of states
* convergence, using different starting values
* simulating data



###Determining the number of hidden states
The first step in developing a HMM is to determine the number of states $m$ that best describes the observed data, and is a model selection problem. In case of the mouse data, the task is to define the states by clusters of observed behavioral acts that provide a reasonable, biologically interpretable, description of the data. As such, we used a combination of unadjusted and the Bayesian Information Criterion (BIC) adjusted log- likelihoods and the biological interpretability of the estimated states to choose between models. We used both the unadjusted and BIC adjusted log-likelihoods, because the likelihood ratio test, commonly used to compare nested models, cannot be used in case of the HMM (i.e., the difference in the log-likelihoods between models is not $\chi^2$ distributed \cite{ryden2008}). The BIC adjusted log-likelihood provides an alternative to compare the log-likelihoods of various models adjusted for the number of estimated parameters in the model. The BIC is defined as BIC $= -2 \text{log} L + n_p \ \text{log}(T)$, where $L$ is the likelihood of the model, $n_p$ is the number of (freely) estimated parameters, and $T$ is the length of the observed event sequence (see the vignette [Estimation of the multilevel hidden Markov model](estimation-mhmm.pdf) for the likelihood function).^[We note, however, that the BIC approximates the posterior distribution of the parameters by a Gaussian distribution, which might not be appropriate for models including parameters on the boundary of the parameter space (e.g., close to 0 or 1 in case of probability estimates), or for small data sets, as exemplified by @scott2002. Model selection is therefore not a straightforward procedure in the context of HMM, and the choices remain subjective.]

###Determining the most likely state sequence 
Given a well-fitting HMM, it may be of interest to determine the actual *sequence*, or order of succession, of hidden states that has most likely given rise to the sequence of outcomes as observed in a subject. One can either use local decoding, in which the probabilities of the hidden state sequence are obtained simultaneously with the model parameters estimates, or the well-known Viterbi algorithm  [@viterbi1967; @forney1973]. In local decoding, the most likely state is determined separately at each time point $t$, in contrast to the Viterbi algorithm in which one determines the joint probability of the complete sequence of observations $O_{1:T}$ and the complete sequence of hidden states $S_{1:T}$.





Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))

## References

